# Daedalus Configuration Example
# Copy this file to ~/.config/daedelus/config.yaml and customize

# ============================================
# Daemon Settings
# ============================================
daemon:
  # Unix socket path for IPC (auto-set if null)
  socket_path: null  # Default: ~/.local/share/daedelus/runtime/daemon.sock

  # Log file path (auto-set if null)
  log_path: null     # Default: ~/.local/share/daedelus/daemon.log

  # PID file path (auto-set if null)
  pid_path: null     # Default: ~/.local/share/daedelus/runtime/daemon.pid

  # Startup delay in seconds
  startup_delay: 0.5

  # Max worker threads for parallel processing
  max_workers: 4

# ============================================
# Embedding Model Settings (Phase 1)
# ============================================
model:
  # Embedding vector dimensionality
  # Higher = more expressive but slower (64, 128, 256, 384)
  embedding_dim: 128

  # Maximum vocabulary size
  vocab_size: 50000

  # Model file path (auto-set if null)
  model_path: null   # Default: ~/.local/share/daedelus/daedelus.bin

  # Minimum word frequency to include in vocabulary
  min_count: 2

  # Maximum length of character n-grams (for subword support)
  # Higher = better typo handling
  word_ngrams: 3

  # Training epochs
  epoch: 5

# ============================================
# Vector Store Settings (Phase 1)
# ============================================
vector_store:
  # Index type: "annoy" (Phase 1) or "sqlite-vss" (Phase 2)
  index_type: annoy

  # Index file path (auto-set if null)
  index_path: null   # Default: ~/.local/share/daedelus/index

  # Number of trees for Annoy index
  # More trees = better accuracy but slower build (5-20 recommended)
  n_trees: 10

  # Search parameter (-1 = auto)
  # Higher = more accurate but slower (-1, 100, 1000, 10000)
  search_k: -1

# ============================================
# Database Settings
# ============================================
database:
  # SQLite database path (auto-set if null)
  path: null         # Default: ~/.local/share/daedelus/history.db

  # Enable automatic backups
  backup_enabled: true

  # Number of backup files to keep
  backup_count: 5

# ============================================
# Privacy & Security Settings
# ============================================
privacy:
  # Directories to exclude from learning
  # Commands run in these paths won't be logged
  excluded_paths:
    - ~/.ssh
    - ~/.gnupg
    - ~/.password-store
    - ~/.config/pass

  # Command patterns to exclude
  # Regular expressions matching these won't be logged
  excluded_patterns:
    - password
    - token
    - secret
    - api[_-]?key
    - aws[_-]?access
    - aws[_-]?secret

  # How long to keep command history (in days)
  # Set to 0 for unlimited
  history_retention_days: 90

  # Encrypt sensitive command data
  encrypt_sensitive: true

# ============================================
# Suggestion Engine Settings
# ============================================
suggestions:
  # Maximum number of suggestions to return
  max_suggestions: 5

  # Minimum confidence threshold (0.0 - 1.0)
  # Lower = more suggestions but less relevant
  min_confidence: 0.3

  # Number of recent commands to consider for context
  context_window: 10

  # Enable fuzzy matching
  enable_fuzzy: true

# ============================================
# Performance Settings
# ============================================
performance:
  # LRU cache size for suggestions
  cache_size: 1000

  # Enable lazy loading of models
  lazy_loading: true

  # Batch size for bulk operations
  batch_size: 100

# ============================================
# Phase 2: LLM Settings
# ============================================
llm:
  # Enable LLM-based suggestions
  enabled: true

  # Path to GGUF model file (TinyLlama 1.1B recommended - 100% FOSS)
  # Automatically scans for any .gguf file in ~/.local/share/models/
  # PRIORITY: deus.gguf > daedelus_v*.gguf > other .gguf files
  model_path: null   # Default: auto-detects models in priority order

  # Context length for LLM
  context_length: 2048

  # Sampling temperature (0.0 - 2.0)
  # Lower = more conservative, Higher = more creative
  temperature: 0.7

  # Nucleus sampling parameter (0.0 - 1.0)
  top_p: 0.9

  # Maximum tokens to generate
  max_tokens: 100

  # Number of GPU layers to offload (0 = CPU only)
  n_gpu_layers: 0

# ============================================
# Phase 2: PEFT Settings
# ============================================
peft:
  # Enable PEFT/LoRA fine-tuning
  enabled: false

  # Path to save LoRA adapters
  adapter_path: null  # Default: ~/.local/share/daedelus/llm/adapter

  # LoRA rank
  r: 8

  # LoRA alpha parameter
  lora_alpha: 32

  # LoRA dropout
  lora_dropout: 0.1

# ============================================
# Deus Model Settings (Continuous Learning)
# ============================================
deus:
  # Enable deus.gguf model system
  # When enabled, deus.gguf is prioritized over other models
  enabled: true

  # Automatic fine-tuning threshold (number of commands)
  # Training triggers automatically when this threshold is reached
  training_threshold: 500

  # Enable manual training command
  # Allows users to trigger training via "daedelus train" command
  manual_training_enabled: true

  # Show training progress notifications
  show_notifications: true

  # Show status bars during training
  show_status_bars: true

  # Prompt for model download if no models available
  auto_download_prompt: true

# ============================================
# Token Compression Settings
# ============================================
compression:
  # Enable semantic token compression
  enabled: true

  # Semantic similarity threshold for chunking (0.0 - 1.0)
  # Higher = more aggressive chunking
  similarity_threshold: 0.75

  # Maximum tokens per chunk
  max_chunk_tokens: 512

  # Minimum sentences per chunk
  min_chunk_sentences: 2

  # Use aggressive compression (higher compression ratio)
  aggressive: false

# ============================================
# Continuous Learning Settings
# ============================================
continuous_learning:
  # Enable continuous learning for embeddings
  # The daedelus embedding model learns from new commands
  enabled: true

  # Minimum new commands before incremental training
  embedding_training_threshold: 100

  # Enable periodic embedding updates
  periodic_updates: true

# ============================================
# Command Safety Settings
# ============================================
safety:
  # Enable command safety analysis
  enabled: true

  # Safety level: off, warn, block
  # - off: No safety checks
  # - warn: Show warnings but allow execution
  # - block: Block dangerous commands
  level: warn

  # Patterns to flag as dangerous (regular expressions)
  dangerous_patterns:
    - 'rm\s+-rf\s+/'
    - 'dd\s+.*of=/dev/'
    - 'mkfs\.'
    - 'chmod\s+-R\s+777'
    - ':(){:|:&};:'  # Fork bomb
    - 'sudo\s+rm'
    - '>\s*/dev/sd'

  # Commands that always require confirmation
  require_confirmation:
    - rm
    - dd
    - mkfs
    - fdisk
    - wipefs

  # Whitelist patterns (never flag these)
  whitelist_patterns:
    - 'rm -rf ./build'
    - 'rm -rf ./dist'

# ============================================
# Advanced Settings
# ============================================
advanced:
  # Enable debug logging
  debug: false

  # Profile performance (creates .prof files)
  profiling: false

  # Validate all input data strictly
  strict_validation: false
